{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Configuration File\n",
    "I'm using the ymal type configuration file to treat the hyper parameters that will be used in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('example_config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.CLoader)\n",
    "\n",
    "branches = config['branches']\n",
    "hyper_params = config['hyper_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily check the hyper parameters or configuration of the training in this file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________hyper_params______________\n",
      " dim_output : 3\n",
      " dim_ffnn : 64\n",
      " num_blocks : 2\n",
      " num_heads : 2\n",
      " depth : 4\n",
      " batch_size : 128\n",
      " learning_rate : 0.0003\n",
      "________________branches________________\n",
      " jet_branches : ['jet_pt', 'jet_eta', 'jet_phi', 'jet_mass', 'jet_b_tag']\n",
      " lep_branches : ['lep_pt', 'lep_eta', 'lep_phi', 'lep_mass', 'lep_charge', 'lep_isMuon']\n",
      " met_branches : ['met', 'met_phi']\n",
      " target_branch : jet_parton_match_detail\n",
      " reco_branches : ['weight']\n",
      "______________loader_args_______________\n",
      " pin_memory : True\n",
      " shuffle : True\n",
      " n_epoch : 10\n"
     ]
    }
   ],
   "source": [
    "def read_config(config):\n",
    "    for key in config:\n",
    "        if isinstance(config[key], dict):\n",
    "            print(f\"{key:_^40}\")\n",
    "            read_config(config[key])\n",
    "        else:\n",
    "            print(f\" {key} : {config[key]}\")\n",
    "\n",
    "read_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Dataset\n",
    "the file that loaded in this example is analyzed ttbar dileptonic channel ($t\\bar{t}\\rightarrow bWbW$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delphes;2', 'delphes;1', 'unmatched;2', 'unmatched;1', 'genWeight;1', 'cutflow;1']\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "\n",
    "example_rootfile = \"example_dilepton.root\"\n",
    "f = uproot.open(example_rootfile)\n",
    "print(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file you can see 'delphes' and 'unmatched' tree, 'delphes' tree contains jet parton matched events otherwise 'unmatched' contains unmatched events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TTbarDileptonDataset] 781 / 781 (100.00 %): : 1it [00:00, 17.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from saja import TTbarDileptonDataset\n",
    "\n",
    "tree_path = 'delphes'\n",
    "dataset = TTbarDileptonDataset(example_rootfile,\n",
    "                               tree_path,\n",
    "                               **config['branches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief view of dataset in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________event0______________________\n",
      "jet: tensor([[86.2265, -0.2301, -1.6474,  9.3401,  1.0000],\n",
      "        [33.6623,  0.1983,  1.0714,  7.3076,  0.0000]])\n",
      "lepton: tensor([[ 5.4675e+01,  4.6465e-01, -2.5411e+00,  0.0000e+00, -1.0000e+00,\n",
      "          1.0000e+00],\n",
      "        [ 5.0344e+01, -7.5076e-01, -3.9229e-01,  9.5367e-07,  1.0000e+00,\n",
      "          1.0000e+00]])\n",
      "met: tensor([77.8212,  1.4392])\n",
      "target: tensor([1, 1])\n",
      "reco: tensor([1.])\n",
      "______________________event1______________________\n",
      "jet: tensor([[154.3780,  -0.4807,  -2.0845,  21.8055,   1.0000],\n",
      "        [ 31.3351,   0.3568,   0.3271,   6.0551,   0.0000]])\n",
      "lepton: tensor([[ 1.0395e+02,  9.9853e-03,  1.5579e+00, -1.3487e-06, -1.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 4.5997e+01, -7.6262e-01,  2.6669e+00,  0.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00]])\n",
      "met: tensor([ 7.3291e+01, -4.0187e-02])\n",
      "target: tensor([1, 1])\n",
      "reco: tensor([1.])\n",
      "______________________event2______________________\n",
      "jet: tensor([[57.9330,  0.7417, -1.3356,  6.8041,  0.0000],\n",
      "        [51.2083, -2.2307,  0.9760,  7.9621,  0.0000]])\n",
      "lepton: tensor([[ 4.1102e+01,  7.2577e-01, -5.8642e-01,  6.7435e-07, -1.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 2.6456e+01, -4.8167e-01, -2.0542e+00,  0.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00]])\n",
      "met: tensor([27.5713,  1.4908])\n",
      "target: tensor([1, 1])\n",
      "reco: tensor([1.])\n",
      "______________________event3______________________\n",
      "jet: tensor([[ 1.0346e+02,  4.9270e-01, -2.4740e+00,  1.8254e+01,  1.0000e+00],\n",
      "        [ 3.7392e+01,  1.6463e-01, -1.5884e-02,  2.4180e+00,  0.0000e+00]])\n",
      "lepton: tensor([[ 1.3076e+02,  1.5269e+00,  5.5917e-01, -3.8147e-06, -1.0000e+00,\n",
      "          1.0000e+00],\n",
      "        [ 9.5924e+01, -7.9800e-01,  3.0853e+00, -1.9073e-06,  1.0000e+00,\n",
      "          1.0000e+00]])\n",
      "met: tensor([54.3837, -0.1124])\n",
      "target: tensor([1, 1])\n",
      "reco: tensor([1.])\n",
      "______________________event4______________________\n",
      "jet: tensor([[ 1.9270e+02, -1.5720e+00, -1.7011e+00,  1.2884e+01,  1.0000e+00],\n",
      "        [ 1.8368e+02, -1.7945e+00,  2.7369e+00, -1.1925e+00,  0.0000e+00],\n",
      "        [ 1.0268e+02, -1.5722e-01,  7.6317e-01,  6.1473e+00,  0.0000e+00]])\n",
      "lepton: tensor([[ 3.6712e+01, -1.6534e+00,  1.1860e+00,  0.0000e+00, -1.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 2.6273e+01, -1.7084e+00, -2.8856e+00,  9.5367e-07,  1.0000e+00,\n",
      "          1.0000e+00]])\n",
      "met: tensor([1.4752e+02, 7.0267e-02])\n",
      "target: tensor([1, 0, 1])\n",
      "reco: tensor([-1.])\n"
     ]
    }
   ],
   "source": [
    "for evt_num, evt in enumerate(dataset[:5]):\n",
    "    print(f\"{'event'+str(evt_num):_^50}\")\n",
    "    for key in evt.keys():\n",
    "        print(f\"{key}: {evt[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and validation dataset\n",
    "In this tutorial I used the same dataset for the train and validation,\n",
    "but you have to use splited dataset for validation in real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "valid_dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we usually scale the variables in the same range \n",
    "\n",
    "E.G. jet pt range [0, 700] --> [0, 1], jet eta range [-2.4, 2.4] --> [0, 1] ...\n",
    "\n",
    "and we have to save the scaler's value after fitting for the training dataset since the scale values must be fitted in $\\textit{\"train\"}$ dataset (Fixed parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from saja import MinMaxScaler\n",
    "\n",
    "save_path = \"model_output/tutorial_model\"\n",
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "        branches['jet_branches'],\n",
    "        branches['lep_branches'],\n",
    "        branches['met_branches'],\n",
    "        )\n",
    "scaler.fit(train_dataset)\n",
    "torch.save(scaler, f'{save_path}/scaler.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fit the scaler in train dataset, we scale the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = torch.load(f'{save_path}/scaler.pt')  # you must use the scaler that fitted in the training dataset\n",
    "scaler.transform(train_dataset)\n",
    "scaler.transform(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader is used for training such as splitting the dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=hyper_params['batch_size'],\n",
    "                          collate_fn=train_dataset.collate,\n",
    "                          **config['loader_args'],\n",
    "                          )\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=512,\n",
    "                          collate_fn=valid_dataset.collate,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before define the model and optimizer, all the training stuffs (batch, model...) should be in the same device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the model with hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saja import TTbarDileptonSAJA\n",
    "\n",
    "model = TTbarDileptonSAJA(dim_jet=len(branches['jet_branches']),\n",
    "                          dim_lepton=len(branches['lep_branches']),\n",
    "                          dim_met=len(branches['met_branches']),\n",
    "                          dim_output=2,  # 0: other, 1: b-parton matched jet\n",
    "                          dim_ffnn=hyper_params['dim_ffnn'],\n",
    "                          num_blocks=hyper_params['num_blocks'],\n",
    "                          num_heads=hyper_params['num_heads'],\n",
    "                          depth=hyper_params['depth'],\n",
    "        ).to(device)  # Send to devcie (GPU or CPU..., whatever you defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(),\n",
    "                       lr=hyper_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saja import object_wise_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, opt, device):\n",
    "    torch.set_grad_enabled(True)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "        # Using custom loss\n",
    "        loss = object_wise_cross_entropy(logits,\n",
    "                                         batch.target,\n",
    "                                         torch.logical_not(\n",
    "                                             batch.jet_data_mask\n",
    "                                             ),\n",
    "                                         batch.jet_lengths)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * len(batch.target)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, device):\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    for batch in valid_loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "        loss = object_wise_cross_entropy(logits,\n",
    "                                         batch.target,\n",
    "                                         torch.logical_not(\n",
    "                                             batch.jet_data_mask\n",
    "                                             ),\n",
    "                                         batch.jet_lengths,\n",
    "                                         reduction='none').sum()\n",
    "        valid_loss += loss.item()\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the train and validation functions are defined, and used in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Done\n",
      "  train_loss = 682.0090\tvalid_loss = 644.4925\n",
      "Epoch 1 Done\n",
      "  train_loss = 590.8404\tvalid_loss = 537.7847\n",
      "Epoch 2 Done\n",
      "  train_loss = 527.0134\tvalid_loss = 482.5629\n",
      "Epoch 3 Done\n",
      "  train_loss = 487.2957\tvalid_loss = 446.5849\n",
      "Epoch 4 Done\n",
      "  train_loss = 457.8244\tvalid_loss = 428.2918\n",
      "Epoch 5 Done\n",
      "  train_loss = 437.1365\tvalid_loss = 420.3568\n",
      "Epoch 6 Done\n",
      "  train_loss = 430.8327\tvalid_loss = 416.9252\n",
      "Epoch 7 Done\n",
      "  train_loss = 425.9379\tvalid_loss = 415.6815\n",
      "Epoch 8 Done\n",
      "  train_loss = 422.8520\tvalid_loss = 415.6044\n",
      "Epoch 9 Done\n",
      "  train_loss = 421.4663\tvalid_loss = 415.0670\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config['n_epoch']):\n",
    "    train_loss = train(model, train_loader, opt, device)\n",
    "    valid_loss = validation(model, valid_loader, device)\n",
    "    print(f\"Epoch {epoch} Done\")\n",
    "    print(f\"  {train_loss = :.4f}\\t{valid_loss = :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saja-dilep-py311",
   "language": "python",
   "name": "saja-dilep-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
